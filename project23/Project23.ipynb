{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:25:31.513422Z",
     "start_time": "2018-03-18T13:25:21.422367Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "spark = SparkSession \\\n",
    "          .builder \\\n",
    "          .appName(\"Project23\") \\\n",
    "          .config(\"hive.metastore.uris\", \"thrift://localhost:9083\") \\\n",
    "          .enableHiveSupport() \\\n",
    "          .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T00:00:04.749703Z",
     "start_time": "2018-03-18T00:00:00.618570Z"
    }
   },
   "outputs": [],
   "source": [
    "# define schema to load data\n",
    "schema = StructType([\n",
    "        StructField(\"year\", IntegerType(), True),\n",
    "        StructField(\"cname113\", StringType(), True),\n",
    "        StructField(\"cname\", StringType(), True),\n",
    "        StructField(\"state\", StringType(), True),\n",
    "        StructField(\"nod\", IntegerType(), True),\n",
    "        StructField(\"avg_age\", FloatType(), True)\n",
    "    ])\n",
    "\n",
    "# load csv file from local to spark\n",
    "csv_df = spark.read.csv(\"/home/cloudera/data.csv\", header=True, mode=\"DROPMALFORMED\", schema=schema)\n",
    "\n",
    "# save data to hive\n",
    "csv_df.createOrReplaceTempView(\"data\")\n",
    "spark.sql(\"CREATE TABLE info \\\n",
    "           (year STRING, cname113 STRING,cname STRING, state STRING, nod INT, avg_age FLOAT)\\\n",
    "           STORED AS ORC\" )    \n",
    "spark.sql(\"INSERT INTO info SELECT * FROM data\")\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:25:51.298706Z",
     "start_time": "2018-03-18T13:25:45.179962Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data from hive into dataframe\n",
    "df = spark.table(\"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T00:06:14.518527Z",
     "start_time": "2018-03-18T00:06:09.027483Z"
    }
   },
   "outputs": [],
   "source": [
    "# Total death by years\n",
    "total_death_by_years = df.select(\"year\", \"nod\") \\\n",
    "                        .groupBy(\"year\") \\\n",
    "                        .sum(\"nod\") \\\n",
    "                        .orderBy(\"year\") \\\n",
    "                        .rdd \\\n",
    "                        .map(lambda x: (x[0], x[1])) \\\n",
    "                        .collect()\n",
    "\n",
    "# total_death_by_years = spark.sql(\"SELECT year, SUM(nod) FROM info GROUP BY year ORDER BY year\") \\\n",
    "#                             .rdd \\\n",
    "#                             .map(lambda x: (x[0], x[1])) \\\n",
    "#                             .collect()\n",
    "\n",
    "data = [go.Bar(\n",
    "            x = list(map(lambda x: x[0], total_death_by_years)),\n",
    "            y = list(map(lambda x: x[1], total_death_by_years)),\n",
    "            marker=dict(color='rgb(158,202,225)',\n",
    "                        line=dict(color='rgb(8,48,107)',width=0.5,)),\n",
    "    )]\n",
    "fig = dict(data=data, layout=dict(title=\"Total Deaths By Year\"))\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:35:13.943364Z",
     "start_time": "2018-03-18T13:35:02.581138Z"
    }
   },
   "outputs": [],
   "source": [
    "# A rate causes of death from 1999 to 2015\n",
    "all_causes_rate = df.select(\"cname\", \"nod\") \\\n",
    "                        .groupBy(\"cname\") \\\n",
    "                        .sum(\"nod\") \\\n",
    "                        .rdd \\\n",
    "                        .map(lambda x: (x[0], x[1])) \\\n",
    "                        .collect()\n",
    "\n",
    "# all_causes_rate = spark.sql(\"SELECT cname, SUM(nod) FROM info GROUP BY cname\") \\\n",
    "#                         .rdd \\\n",
    "#                         .map(lambda x: (x[0], x[1])) \\\n",
    "#                         .collect()\n",
    "\n",
    "data = [go.Pie(\n",
    "            labels = list(map(lambda x: x[0], all_causes_rate)),\n",
    "            values = list(map(lambda x: x[1], all_causes_rate)),\n",
    "    )]\n",
    "fig = dict(data=data, layout=dict(title=\"Death Causes Rate\"))\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T00:56:09.965569Z",
     "start_time": "2018-03-18T00:55:58.392481Z"
    }
   },
   "outputs": [],
   "source": [
    "# Total death by states\n",
    "\n",
    "# define schema to load data\n",
    "us_schema = StructType([\n",
    "        StructField(\"code\", StringType(), True),\n",
    "        StructField(\"state\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "# load csv file from local\n",
    "us_df = spark.read.csv(\"/home/cloudera/us.csv\", header=True, mode=\"DROPMALFORMED\", schema=us_schema)\n",
    "\n",
    "total_death_by_states = df.select(\"state\", \"nod\", \"year\", \"cname\") \\\n",
    "                        .groupBy(\"state\") \\\n",
    "                        .agg(sum(\"nod\").alias(\"sod\")) \\\n",
    "                        .join(us_df, us_df.state==df.state) \\\n",
    "                        .select(\"code\", \"sod\") \\\n",
    "                        .rdd \\\n",
    "                        .map(lambda x: (x[0], x[1])) \\\n",
    "                        .collect()\n",
    "\n",
    "# us_df.createOrReplaceTempView(\"us\")\n",
    "# spark.sql(\"SELECT u.code, sum(i.nod) AS sod FROM info AS i JOIN us AS u ON i.state = u.state GROUP BY u.code\")\n",
    "#         .rdd \\\n",
    "#         .map(lambda x: (x[0], x[1])) \\\n",
    "#         .collect()\n",
    "                        \n",
    "# We can add condition to show different:\n",
    "# .where(df.year == \"1999\") \\\n",
    "# or .where(\"df.cname == \"Suicide\") \\\n",
    "\n",
    "data = [dict(\n",
    "        type='choropleth',\n",
    "        autocolorscale = True,\n",
    "        locations = list(map(lambda x: x[0], total_death_by_states)),\n",
    "        z = list(map(lambda x: x[1], total_death_by_states)),\n",
    "        locationmode = 'USA-states',\n",
    "        marker = dict(\n",
    "            line = dict (\n",
    "                color = 'rgb(255,255,255)',\n",
    "                width = 2\n",
    "            ) ),\n",
    "        colorbar = dict(\n",
    "            title = \"No of Death\")\n",
    "        )]\n",
    "layout = dict(\n",
    "        title = 'Number of Death by State (1999-2015)',\n",
    "        geo = dict(\n",
    "            scope='usa',\n",
    "            projection=dict(type='albers usa'),\n",
    "            showlakes = True,\n",
    "            lakecolor = 'rgb(255, 255, 255)'),\n",
    "            )\n",
    "    \n",
    "fig = dict(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-17T23:33:44.105233Z",
     "start_time": "2018-03-17T23:33:43.728438Z"
    }
   },
   "outputs": [],
   "source": [
    "fnt = spark.sql(\"show functions\").collect()\n",
    "spark.sql(\"describe function instr\").show(truncate = False)\n",
    "spark.sql(\"show databases\").show()\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
